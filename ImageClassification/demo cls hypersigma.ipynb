{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:01:47.986946Z",
     "start_time": "2024-11-19T09:01:43.143881Z"
    }
   },
   "source": [
    "from model import ss_fusion_cls\n",
    "import torch\n",
    "from torch  import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,cohen_kappa_score\n",
    "from model import split_data,utils\n",
    "from sklearn import metrics, preprocessing\n",
    "from mmengine.optim import build_optim_wrapper\n",
    "from mmcv_custom import custom_layer_decay_optimizer_constructor,layer_decay_optimizer_constructor_vit\n",
    "import scipy.io as sio\n",
    "from thop import profile\n",
    "from multiprocessing import shared_memory"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsq/anaconda3/envs/hypersigma/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/wsq/anaconda3/envs/hypersigma/lib/python3.8/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:20.023998Z",
     "start_time": "2024-11-19T09:02:20.017681Z"
    }
   },
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:23.325258Z",
     "start_time": "2024-11-19T09:02:23.315598Z"
    }
   },
   "source": [
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.data_cube = None\n",
    "        self.g_truth = None\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        \"\"\"\n",
    "        origin data\n",
    "        \"\"\"\n",
    "        return self.data_cube\n",
    "\n",
    "    @property\n",
    "    def truth(self):\n",
    "        return self.g_truth\n",
    "\n",
    "    @property\n",
    "    def normal_cube(self):\n",
    "        \"\"\"\n",
    "        normalization data: range(0, 1)\n",
    "        \"\"\"\n",
    "        return (self.data_cube - np.min(self.data_cube)) / (np.max(self.data_cube) - np.min(self.data_cube))\n",
    "class IndianRaw(DataReader):\n",
    "    def __init__(self):\n",
    "        super(IndianRaw, self).__init__()\n",
    "        raw_data_package = sio.loadmat(r\"data/Indian_pines_corrected.mat\")\n",
    "        self.data_cube = raw_data_package[\"data\"].astype(np.float32)\n",
    "        truth = sio.loadmat(r\"data/Indian_pines_gt.mat\")\n",
    "        self.g_truth = truth[\"groundT\"].astype(np.float32)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:26.302701Z",
     "start_time": "2024-11-19T09:02:26.299018Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    data = IndianRaw().cube\n",
    "    data_gt = IndianRaw().truth\n",
    "    return data, data_gt\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:28.237705Z",
     "start_time": "2024-11-19T09:02:27.844315Z"
    }
   },
   "source": [
    "img_size =33\n",
    "patch_size = 2\n",
    "pca_components = 30\n",
    "split_type = ['number', 'ratio'][0]\n",
    "train_num =10\n",
    "val_num =5\n",
    "train_ratio = 0.98 \n",
    "val_ratio = 0.01 \n",
    "\n",
    "max_epoch = 100\n",
    "batch_size = 16\n",
    "learning_rate = 0.00001 \n",
    "dataset_name = 'HH'\n",
    "\n",
    "path_weight = r\"/home5/wsq/HyperSIGMA/ImageClassification/weights/\"\n",
    "path_result = r\"/home5/wsq/HyperSIGMA/ImageClassification/result/\"\n",
    "data, data_gt = load_data()\n",
    "height, width, bands = data.shape\n",
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(data_gt)\n",
    "class_num = class_num.astype(int)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:30.789471Z",
     "start_time": "2024-11-19T09:02:30.649124Z"
    }
   },
   "source": [
    "data, data_gt = load_data()\n",
    "train_index, val_index, test_index = split_data.split_data(gt_reshape, \n",
    "            class_num, train_ratio, train_ratio, train_num, val_num, split_type)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:35.535719Z",
     "start_time": "2024-11-19T09:02:32.455795Z"
    }
   },
   "source": [
    "data, pca = split_data.apply_PCA(data, num_components=pca_components)\n",
    "data_all, data_label_all = split_data.create_patches(data, data_gt, window_size=img_size, remove_zero_labels = False)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:36.864474Z",
     "start_time": "2024-11-19T09:02:36.855501Z"
    }
   },
   "source": [
    "train_index=train_index.astype(int)\n",
    "val_index=val_index.astype(int)\n",
    "test_index=test_index.astype(int)\n",
    "height, width, bands = data.shape\n",
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:41.339530Z",
     "start_time": "2024-11-19T09:02:38.731020Z"
    }
   },
   "source": [
    "train_index0, val_index0, re_index = split_data.split_data(gt_reshape, \n",
    "            class_num, 0, 0, 0, 10,'ratio' )\n",
    "re_index = np.sort(re_index)\n",
    "\n",
    "train_index = train_index.reshape(-1,1)\n",
    "val_index = val_index.reshape(-1,1)\n",
    "test_index = test_index.reshape(-1,1)\n",
    "\n",
    "train_label = data_label_all[train_index].reshape(-1,)\n",
    "val_label = data_label_all[val_index].reshape(-1,)\n",
    "test_label = data_label_all[test_index]\n",
    "\n",
    "train = data_all[train_index,:,:,:]\n",
    "val = data_all[val_index,:,:,:]\n",
    "test = data_all[test_index,:,:,:]\n",
    "\n",
    "split_data.data_info(train_label, val_label, test_label, np.max(data_label_all))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 \t 0 \t 0 \t 0\n",
      "class 1 \t 10 \t 5 \t 31\n",
      "class 2 \t 10 \t 5 \t 1413\n",
      "class 3 \t 10 \t 5 \t 815\n",
      "class 4 \t 10 \t 5 \t 222\n",
      "class 5 \t 10 \t 5 \t 468\n",
      "class 6 \t 10 \t 5 \t 715\n",
      "class 7 \t 10 \t 5 \t 13\n",
      "class 8 \t 10 \t 5 \t 463\n",
      "class 9 \t 10 \t 5 \t 5\n",
      "class 10 \t 10 \t 5 \t 957\n",
      "class 11 \t 10 \t 5 \t 2440\n",
      "class 12 \t 10 \t 5 \t 578\n",
      "class 13 \t 10 \t 5 \t 190\n",
      "class 14 \t 10 \t 5 \t 1250\n",
      "class 15 \t 10 \t 5 \t 371\n",
      "class 16 \t 10 \t 5 \t 78\n",
      "total     \t 160 \t 80 \t 10009\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:52.383773Z",
     "start_time": "2024-11-19T09:02:52.376905Z"
    }
   },
   "source": [
    "train = train.reshape(-1, img_size, img_size, bands)\n",
    "val = val.reshape(-1, img_size, img_size, bands)\n",
    "test = test.reshape(-1,img_size, img_size, bands)\n",
    "print('before transpose: train shape: ', train.shape)\n",
    "print('before transpose: test  shape: ', val.shape)\n",
    "print('before transpose: test  shape: ', test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before transpose: train shape:  (160, 33, 33, 30)\n",
      "before transpose: test  shape:  (80, 33, 33, 30)\n",
      "before transpose: test  shape:  (10009, 33, 33, 30)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:55.076147Z",
     "start_time": "2024-11-19T09:02:55.071661Z"
    }
   },
   "source": [
    "train = train.transpose(0, 3, 1, 2)\n",
    "val = val.transpose(0,  3, 1,2 )\n",
    "test = test.transpose(0,  3, 1,2 )\n",
    "print('after transpose: train shape: ', train.shape)\n",
    "print('after transpose: val  shape: ', val.shape)\n",
    "print('after transpose: test  shape: ', test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after transpose: train shape:  (160, 30, 33, 33)\n",
      "after transpose: val  shape:  (80, 30, 33, 33)\n",
      "after transpose: test  shape:  (10009, 30, 33, 33)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:02:57.039685Z",
     "start_time": "2024-11-19T09:02:56.971161Z"
    }
   },
   "source": [
    "train_hi_gt= torch.zeros(train_label.size,class_num)\n",
    "val_hi_gt= torch.zeros(val_label.size,class_num)\n",
    "test_hi_gt= torch.zeros(test_label.size,class_num)\n",
    "\n",
    "\n",
    "train_hi_gt = torch.LongTensor(train_hi_gt.numpy())\n",
    "test_hi_gt = torch.LongTensor(test_hi_gt.numpy())\n",
    "val_hi_gt = torch.LongTensor(val_hi_gt.numpy())\n",
    "\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train.shape[0]\n",
    "        self.x_data = torch.FloatTensor(train)\n",
    "        self.y_data = torch.LongTensor(train_label)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Val dataset\"\"\"\n",
    "\n",
    "\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val.shape[0]\n",
    "        self.x_data = torch.FloatTensor(val)\n",
    "        self.y_data = torch.LongTensor(val_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "\n",
    "\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = test.shape[0]\n",
    "        self.x_data = torch.FloatTensor(test)\n",
    "        self.y_data = torch.LongTensor(test_label)  \n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "valset = ValDS()\n",
    "testset = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=32, shuffle=False, num_workers=0)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:03:03.825979Z",
     "start_time": "2024-11-19T09:03:00.805387Z"
    }
   },
   "source": [
    "model = ss_fusion_cls.SSFusionFramework(\n",
    "                img_size = img_size,\n",
    "                in_channels = pca_components,\n",
    "                patch_size=patch_size,\n",
    "                classes = class_num+1,\n",
    "                model_size='base' #The optional values are 'base','large' and 'huge'\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:03:54.336853Z",
     "start_time": "2024-11-19T09:03:21.949887Z"
    }
   },
   "source": [
    "model_params =model.state_dict()\n",
    "spat_net = torch.load((r\"/home5/wsq/HyperSIGMA/spat-base.pth\"), map_location=torch.device('cuda'))\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_map' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_output_maps' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'pos_embed' in k:\n",
    "        del spat_net['model'][k]\n",
    "spat_weights = {}\n",
    "prefix = 'spat_encoder.'\n",
    "for key, value in spat_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spat_weights[new_key] = value\n",
    "per_net = torch.load((r\"/home5/wsq/HyperSIGMA/spec-base.pth\"), map_location=torch.device('cuda'))\n",
    "model_params =model.state_dict()\n",
    "for k in list(per_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'spat_map' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'fpn1.0.weight' in k:\n",
    "        del per_net['model'][k]\n",
    "spec_weights = {}\n",
    "prefix = 'spec_encoder.'\n",
    "for key, value in per_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spec_weights[new_key] = value\n",
    "model_params =model.state_dict()\n",
    "for k in list(spec_weights.keys()):\n",
    "    if 'spec_encoder.patch_embed' in k:\n",
    "        del spec_weights[k]\n",
    "merged_params = {**spat_weights, **spec_weights}\n",
    "same_parsms = {k: v for k, v in merged_params.items() if k in model_params.keys()}\n",
    "model_params.update(same_parsms)\n",
    "model.load_state_dict(model_params)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1678656/1626247618.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spat_net = torch.load((r\"/home5/wsq/HyperSIGMA/spat-base.pth\"), map_location=torch.device('cuda'))\n",
      "/tmp/ipykernel_1678656/1626247618.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  per_net = torch.load((r\"/home5/wsq/HyperSIGMA/spec-base.pth\"), map_location=torch.device('cuda'))\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 11.31 MiB is free. Process 3894991 has 257.00 MiB memory in use. Process 2675314 has 257.00 MiB memory in use. Process 1589778 has 40.91 GiB memory in use. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.13 GiB is allocated by PyTorch, and 212.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m     new_key \u001B[38;5;241m=\u001B[39m prefix \u001B[38;5;241m+\u001B[39m key\n\u001B[1;32m     19\u001B[0m     spat_weights[new_key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m---> 20\u001B[0m per_net \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home5/wsq/HyperSIGMA/spec-base.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m model_params \u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mstate_dict()\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(per_net[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys()):\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:1097\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1095\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1096\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1097\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1099\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1100\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1101\u001B[0m \u001B[43m            \u001B[49m\u001B[43moverall_storage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverall_storage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1102\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n\u001B[1;32m   1105\u001B[0m     f_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(f, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:1525\u001B[0m, in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_thread_local_state\u001B[38;5;241m.\u001B[39mmap_location \u001B[38;5;241m=\u001B[39m map_location\n\u001B[0;32m-> 1525\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_thread_local_state\u001B[38;5;241m.\u001B[39mmap_location\n\u001B[1;32m   1528\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:1492\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m   1490\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1491\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[0;32m-> 1492\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1494\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:1466\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[0;34m(dtype, numel, key, location)\u001B[0m\n\u001B[1;32m   1461\u001B[0m         storage\u001B[38;5;241m.\u001B[39mbyteswap(dtype)\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[1;32m   1464\u001B[0m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[1;32m   1465\u001B[0m typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[0;32m-> 1466\u001B[0m     wrap_storage\u001B[38;5;241m=\u001B[39m\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1467\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m   1468\u001B[0m     _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typed_storage\u001B[38;5;241m.\u001B[39m_data_ptr() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1471\u001B[0m     loaded_storages[key] \u001B[38;5;241m=\u001B[39m typed_storage\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:1392\u001B[0m, in \u001B[0;36m_get_restore_location.<locals>.restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestore_location\u001B[39m(storage, location):\n\u001B[0;32m-> 1392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_restore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:414\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    412\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_restore_location\u001B[39m(storage, location):\n\u001B[1;32m    413\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[0;32m--> 414\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    415\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    416\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/serialization.py:392\u001B[0m, in \u001B[0;36m_deserialize\u001B[0;34m(backend_name, obj, location)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m location\u001B[38;5;241m.\u001B[39mstartswith(backend_name):\n\u001B[1;32m    391\u001B[0m     device \u001B[38;5;241m=\u001B[39m _validate_device(location, backend_name)\n\u001B[0;32m--> 392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/storage.py:187\u001B[0m, in \u001B[0;36m_StorageBase.to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m, device: torch\u001B[38;5;241m.\u001B[39mdevice, non_blocking: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:  \u001B[38;5;66;03m# type: ignore[type-var, misc] # noqa: E704\u001B[39;00m\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_to\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/hypersigma/lib/python3.8/site-packages/torch/_utils.py:89\u001B[0m, in \u001B[0;36m_to\u001B[0;34m(self, device, non_blocking)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m     87\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_sparse\n\u001B[1;32m     88\u001B[0m     ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse storage is not supported for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;241m.\u001B[39mtype\u001B[38;5;241m.\u001B[39mupper()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 89\u001B[0m     untyped_storage \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     untyped_storage\u001B[38;5;241m.\u001B[39mcopy_(\u001B[38;5;28mself\u001B[39m, non_blocking)\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m untyped_storage\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 44.56 GiB of which 11.31 MiB is free. Process 3894991 has 257.00 MiB memory in use. Process 2675314 has 257.00 MiB memory in use. Process 1589778 has 40.91 GiB memory in use. Including non-PyTorch memory, this process has 3.14 GiB memory in use. Of the allocated memory 2.13 GiB is allocated by PyTorch, and 212.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "    type='AdamW', lr=6e-5, betas=(0.9, 0.999), weight_decay=0.05),\n",
    "    constructor='LayerDecayOptimizerConstructor_ViT', \n",
    "    paramwise_cfg=dict(\n",
    "        num_layers=12, \n",
    "        layer_decay_rate=0.9,\n",
    "        )\n",
    "        )\n",
    "optimizer = build_optim_wrapper(model, optim_wrapper)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.optimizer, 300, eta_min=0, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "count = 0\n",
    "best_loss = 99999\n",
    "train_losses = []\n",
    "val_losses = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for epoch in range(max_epoch+ 1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    _train_loss = 0\n",
    "    val_correct = 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        _train_loss += train_loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct = (y_pred == y).sum().item()\n",
    "            total = y.size(0)\n",
    "\n",
    "    train_losses.append(train_loss.cpu().detach().item())\n",
    "    \n",
    "    if epoch % 10==0:\n",
    "       \n",
    "        epoch_loss = _train_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "\n",
    "            # 检查并创建目录\n",
    "            #os.makedirs(path_weight, exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), path_weight + \"model.pth\")\n",
    "            torch.save(optimizer.state_dict(), path_weight + 'optimizer.pth')\n",
    "            print('save model')\n",
    "\n",
    "        \n",
    "        print('epoch', epoch,\n",
    "            'loss: ', round(epoch_loss, 5),\n",
    "            'accuracy: ', round(epoch_accuracy, 5),\n",
    "            )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "count = 0\n",
    "model.load_state_dict(torch.load(path_weight + r\"model.pth\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y  in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        test_correct += (y_pred == y).sum().item()\n",
    "        test_total += y.size(0)\n",
    "\n",
    "        if count == 0:\n",
    "            y_pred_test =  y_pred.cpu().numpy()\n",
    "            y_gt_test = y.cpu().numpy()\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred_test = np.concatenate( (y_pred_test, y_pred.cpu().numpy()) )\n",
    "            y_gt_test = np.concatenate( (y_gt_test, y.cpu().numpy()) )\n",
    "\n",
    "overall_acc = metrics.accuracy_score(y_pred_test, y_gt_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_pred_test, y_gt_test)\n",
    "each_acc, average_acc = utils.aa_and_each_accuracy(confusion_matrix)\n",
    "kappa = metrics.cohen_kappa_score(y_pred_test, y_gt_test)\n",
    "OA_HybirdSN = test_correct / test_total\n",
    "print(\"test OA={:.4f}\".format(overall_acc))\n",
    "print('kappa=',kappa)\n",
    "print('each_acc=',each_acc)\n",
    "print('average_acc=',average_acc)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
